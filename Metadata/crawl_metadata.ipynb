{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def wait(): time.sleep(0.5)\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self) -> None:\n",
    "        driver = Chrome()\n",
    "        driver.get(\"https://dart.fss.or.kr/dsab007/main.do?option=report\")\n",
    "\n",
    "        # Input report name\n",
    "        reportName = driver.find_element(By.ID, \"reportName\")\n",
    "        reportName.clear()\n",
    "        reportName.send_keys(\"사업보고서\")\n",
    "        wait()\n",
    "\n",
    "        # Click final report button\n",
    "        finalReport = driver.find_element(By.ID, \"finalReport\")\n",
    "        finalReport.click()\n",
    "        wait()\n",
    "\n",
    "        # Set number line per page to 100\n",
    "        numLinePerPage = driver.find_element(By.ID, \"maxResultsCb\")\n",
    "        Select(numLinePerPage).select_by_value(\"100\")\n",
    "        wait()\n",
    "\n",
    "        #\n",
    "        self.driver = driver\n",
    "\n",
    "    def search_by_startDate_endDate(self, startDate, endDate):\n",
    "        startDate_element = self.driver.find_element(By.ID, \"startDate\")\n",
    "        startDate_element.clear()\n",
    "        startDate_element.send_keys(startDate)\n",
    "        endDate_element = self.driver.find_element(By.ID, \"endDate\")\n",
    "        endDate_element.clear()\n",
    "        endDate_element.send_keys(endDate)\n",
    "        wait()\n",
    "\n",
    "        # Click search button\n",
    "        self.driver.find_element(By.CLASS_NAME, \"btnArea\").find_element(By.CLASS_NAME, \"btnSearch\").click()\n",
    "        wait()\n",
    "\n",
    "        # Wait for the search results to load\n",
    "        self.wait_for_results()\n",
    "\n",
    "    def get_soup(self):\n",
    "        return BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "\n",
    "    def wait_for_results(self, first_index_of_table=1):\n",
    "        while True:\n",
    "            soup = self.get_soup()\n",
    "            table = soup.find(\"table\")\n",
    "            df = pd.read_html(StringIO(str(table)))[0]\n",
    "            if df.iloc[0,0] == first_index_of_table: return\n",
    "            wait()\n",
    "\n",
    "    def get_table(self):\n",
    "        soup = self.get_soup()\n",
    "        table = soup.find(\"table\")\n",
    "        df = pd.read_html(StringIO(str(table)))[0]\n",
    "        list_tr = table.find(\"tbody\").find_all(\"tr\")\n",
    "        assert len(list_tr) == df.shape[0]\n",
    "\n",
    "        list_popup_href = []\n",
    "        list_report_id = []\n",
    "        for tr in list_tr:\n",
    "            list_td = tr.find_all(\"td\")\n",
    "            assert len(list_td) == df.shape[1]\n",
    "\n",
    "            popup_href = list_td[1].find(\"a\")[\"href\"]\n",
    "            report_id = list_td[2].find(\"a\")[\"href\"]\n",
    "            list_popup_href.append(popup_href)\n",
    "            list_report_id.append(report_id)\n",
    "\n",
    "        df[\"popup_href\"] = list_popup_href\n",
    "        df[\"report_id\"] = list_report_id\n",
    "        return df\n",
    "\n",
    "    def go_to_page(self, page_number):\n",
    "        if page_number == 1: return\n",
    "\n",
    "        pageSkip_element = self.driver.find_element(By.CLASS_NAME, \"pageSkip\")\n",
    "        list_li = pageSkip_element.find_element(By.TAG_NAME, \"ul\").find_elements(By.TAG_NAME, \"li\")\n",
    "\n",
    "        if page_number % 10 == 1: button_id = -2\n",
    "        else: button_id = (page_number - 1) % 10 + 2\n",
    "        list_li[button_id].find_element(By.TAG_NAME, \"a\").click()\n",
    "\n",
    "        first_index_of_table = 1 + 100 * (page_number - 1)\n",
    "        self.wait_for_results(first_index_of_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler()\n",
    "for iii in range(4):\n",
    "    startDate = f\"{2022-3*iii}0101\"\n",
    "    endDate = f\"{2024-3*iii}1231\"\n",
    "    # print(f\"Start date: {startDate}, End date: {endDate}\")\n",
    "    crawler.search_by_startDate_endDate(startDate, endDate)\n",
    "    soup = crawler.get_soup()\n",
    "    pageInfo = soup.find(\"div\", attrs={\"class\":\"pageInfo\"})\n",
    "    temp = \"\".join(pageInfo.text.split()).split(\"][\")\n",
    "    total_page = int(temp[0].split(\"/\")[1])\n",
    "    total_line = int(re.findall(r\"\\d+\", \"\".join(temp[1].split(\",\")))[0])\n",
    "    total_df = None\n",
    "    for page in tqdm(range(1, total_page+1)):\n",
    "        crawler.go_to_page(page)\n",
    "        df = crawler.get_table()\n",
    "        try: total_df = pd.concat([total_df, df])\n",
    "        except: total_df = df\n",
    "\n",
    "    total_df.to_csv(f\"{iii}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
